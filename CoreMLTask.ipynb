{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "o0mwISxzvmmR"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import CIFAR100\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-y6MOEM2w3b3",
        "outputId": "58eba63d-ea0a-48f1-c0e1-8ae2019278ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to data/cifar-100-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 169001437/169001437 [00:05<00:00, 28957589.91it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/cifar-100-python.tar.gz to data/\n"
          ]
        }
      ],
      "source": [
        "hflip = transforms.RandomHorizontalFlip(p=0.5)\n",
        "train_transform = transforms.Compose([hflip, transforms.ToTensor(), transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])])\n",
        "dataset = CIFAR100(root = 'data/', download = True, transform = train_transform)\n",
        "test_ds = CIFAR100(root = 'data/', train = False)\n",
        "train_ds, val_ds = random_split(dataset, [40000, 10000])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "-tYOfH5fzFBO"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda:0\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "98Ew85f11e9K"
      },
      "outputs": [],
      "source": [
        "class VanillaSoftmax(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "  def forward(self, x, dim=-1):\n",
        "    return torch.softmax(x, dim=dim)\n",
        "\n",
        "class GumbelSoftmax(nn.Module):\n",
        "  def __init__(self):\n",
        "      super().__init__()\n",
        "      self.linear = nn.Linear(100, 100)\n",
        "      self.temperature = 1\n",
        "\n",
        "  def forward(self, x):\n",
        "      logits = self.linear(x)\n",
        "      return nn.functional.gumbel_softmax(logits, tau=self.temperature)\n",
        "\n",
        "class AdaptiveSoftmax(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.adapt_softmax = nn.AdaptiveLogSoftmaxWithLoss(100, 100, cutoffs=[49, 51], device=\"cuda:0\")  #since all the classes have equal probability, all of them belong to the head cluster\n",
        "\n",
        "  def forward(self, x, y):\n",
        "    return self.adapt_softmax(x, y)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy(outputs, labels):\n",
        "    _, preds = torch.max(outputs, dim=1)\n",
        "    return torch.tensor(torch.sum(preds == labels).item() / len(preds))\n",
        "\n",
        "def to_device(data, device):\n",
        "    if isinstance(data, (list,tuple)):\n",
        "        return [to_device(x, device) for x in data]\n",
        "    return data.to(device, non_blocking=True)\n",
        "\n",
        "class DeviceDataLoader():\n",
        "    def __init__(self, dl, device):\n",
        "        self.dl = dl\n",
        "        self.device = device\n",
        "        \n",
        "    def __iter__(self):\n",
        "        for b in self.dl: \n",
        "            yield to_device(b, self.device)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dl)"
      ],
      "metadata": {
        "id": "kXv7RD0hQpJM"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CifarCNN(nn.Module):\n",
        "    def __init__(self, softmax_type):\n",
        "        super().__init__()\n",
        "        self.network = nn.Sequential(\n",
        "          nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
        "          nn.ReLU(),\n",
        "          nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
        "          nn.ReLU(),\n",
        "          nn.MaxPool2d(2, 2),\n",
        "\n",
        "          nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
        "          nn.ReLU(),\n",
        "          nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1),\n",
        "          nn.ReLU(),\n",
        "          nn.MaxPool2d(2, 2),\n",
        "\n",
        "          nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
        "          nn.ReLU(),\n",
        "          nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
        "          nn.ReLU(),\n",
        "          nn.MaxPool2d(2, 2),\n",
        "\n",
        "          nn.Flatten(), \n",
        "          nn.Linear(256*4*4, 1024),\n",
        "          nn.ReLU(),\n",
        "          nn.Linear(1024, 512),\n",
        "          nn.ReLU(),\n",
        "          nn.Linear(512, 100))\n",
        "        \n",
        "        self.norm_fn = softmax_type\n",
        "        \n",
        "    def forward(self, x):\n",
        "        return self.network(x)\n",
        "\n",
        "    def train_batch(self, batch):\n",
        "\n",
        "      images, targets = batch\n",
        "      preds = self(images)\n",
        "      norm_func = self.norm_fn\n",
        "\n",
        "      if norm_func == \"VanillaSoftmax\":\n",
        "        m = VanillaSoftmax()\n",
        "      elif norm_func == \"GumbelSoftmax\":\n",
        "        m = GumbelSoftmax()\n",
        "      elif norm_func == \"AdaptiveSoftmax\":\n",
        "        m = AdaptiveSoftmax()\n",
        "\n",
        "      if norm_func != \"AdaptiveSoftmax\":\n",
        "        norm_preds = m(preds)\n",
        "        loss = nn.CrossEntropyLoss()\n",
        "        loss = loss(preds, targets)\n",
        "\n",
        "      else:\n",
        "        _, loss = m(preds, targets)\n",
        "      \n",
        "      return loss\n",
        "\n",
        "    def val_step(self, batch):\n",
        "      images, targets = batch\n",
        "      preds = self(images)\n",
        "\n",
        "      norm_func = self.norm_fn\n",
        "\n",
        "      if norm_func == \"VanillaSoftmax\":\n",
        "        m = VanillaSoftmax()\n",
        "      elif norm_func == \"GumbelSoftmax\":\n",
        "        m = GumbelSoftmax()\n",
        "      elif norm_func == \"AdaptiveSoftmax\":\n",
        "        m = AdaptiveSoftmax()\n",
        "\n",
        "      if norm_func != \"AdaptiveSoftmax\":\n",
        "        norm_preds = m(preds)\n",
        "        loss = nn.CrossEntropyLoss()\n",
        "        loss = loss(preds, targets)\n",
        "      else:\n",
        "        _, loss = m(preds, targets)\n",
        "      val_acc = accuracy(preds, targets)\n",
        "\n",
        "      return {'val_loss': loss.detach(), 'val_acc': val_acc}\n",
        "\n",
        "    def val_epoch_status(self, outputs):\n",
        "        batch_losses = [x['val_loss'] for x in outputs]\n",
        "        epoch_loss = torch.stack(batch_losses).mean()   \n",
        "        batch_accs = [x['val_acc'] for x in outputs]\n",
        "        epoch_acc = torch.stack(batch_accs).mean()     \n",
        "        \n",
        "        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n",
        "    \n",
        "    def epoch_status(self, epoch, result):\n",
        "        print(\"Epoch [{}], train_loss: {:.4f}, val_loss: {:.4f}, val_acc: {:.4f}\".format(\n",
        "            epoch, result['train_loss'], result['val_loss'], result['val_acc']))"
      ],
      "metadata": {
        "id": "T_ehGXFpLmy7"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, val_loader):\n",
        "  model.eval()\n",
        "  torch.no_grad()\n",
        "  outputs = [model.val_step(batch) for batch in val_loader]\n",
        "  return model.val_epoch_status(outputs)\n",
        "\n",
        "\n",
        "def fit(model, train_loader, val_loader, epochs=20, lr=0.001, opt_func=torch.optim.Adam):\n",
        "    history = []\n",
        "    optimizer = opt_func(model.parameters(), lr)\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        train_losses = []\n",
        "        for batch in train_loader:\n",
        "            loss = model.train_batch(batch)\n",
        "            train_losses.append(loss)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "        result = evaluate(model, val_loader)\n",
        "        result['train_loss'] = torch.stack(train_losses).mean().item()\n",
        "        model.epoch_status(epoch, result)\n",
        "        history.append(result)"
      ],
      "metadata": {
        "id": "5rRRAczqR8NN"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "64O208onYyiS"
      },
      "outputs": [],
      "source": [
        "train_loader = DataLoader(train_ds, num_workers=2, batch_size=128, pin_memory=True)\n",
        "val_loader = DataLoader(val_ds, num_workers=2, batch_size=128, pin_memory=True)\n",
        "test_loader = DataLoader(test_ds, num_workers=2, batch_size=128, pin_memory=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "oiQEgv0mUtPh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3382e657-c552-4726-eab9-483d617f8f84"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CifarCNN(\n",
              "  (network): Sequential(\n",
              "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): ReLU()\n",
              "    (2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (3): ReLU()\n",
              "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (6): ReLU()\n",
              "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (8): ReLU()\n",
              "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (11): ReLU()\n",
              "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (13): ReLU()\n",
              "    (14): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (15): Flatten(start_dim=1, end_dim=-1)\n",
              "    (16): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "    (17): ReLU()\n",
              "    (18): Linear(in_features=1024, out_features=512, bias=True)\n",
              "    (19): ReLU()\n",
              "    (20): Linear(in_features=512, out_features=100, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "model1 = CifarCNN(\"VanillaSoftmax\")\n",
        "to_device(model1, device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DeviceDataLoader(train_loader, device)\n",
        "val_loader = DeviceDataLoader(val_loader, device)\n",
        "test_loader = DeviceDataLoader(val_loader, device)"
      ],
      "metadata": {
        "id": "ddE8lrOYRKdZ"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fit(model1, train_loader, val_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jJqM7whrTWf7",
        "outputId": "7d4bb177-a2b6-46ad-ec6d-1ac7a64c12bd"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [0], train_loss: 4.2820, val_loss: 3.9340, val_acc: 0.0789\n",
            "Epoch [1], train_loss: 3.6580, val_loss: 3.4377, val_acc: 0.1672\n",
            "Epoch [2], train_loss: 3.2363, val_loss: 3.1203, val_acc: 0.2274\n",
            "Epoch [3], train_loss: 2.9260, val_loss: 2.9019, val_acc: 0.2725\n",
            "Epoch [4], train_loss: 2.6888, val_loss: 2.7645, val_acc: 0.2984\n",
            "Epoch [5], train_loss: 2.4809, val_loss: 2.6493, val_acc: 0.3279\n",
            "Epoch [6], train_loss: 2.3038, val_loss: 2.5819, val_acc: 0.3428\n",
            "Epoch [7], train_loss: 2.1527, val_loss: 2.5595, val_acc: 0.3505\n",
            "Epoch [8], train_loss: 2.0315, val_loss: 2.5513, val_acc: 0.3571\n",
            "Epoch [9], train_loss: 1.9071, val_loss: 2.5746, val_acc: 0.3617\n",
            "Epoch [10], train_loss: 1.8047, val_loss: 2.5210, val_acc: 0.3776\n",
            "Epoch [11], train_loss: 1.7088, val_loss: 2.6438, val_acc: 0.3641\n",
            "Epoch [12], train_loss: 1.6293, val_loss: 2.6471, val_acc: 0.3758\n",
            "Epoch [13], train_loss: 1.5677, val_loss: 2.6753, val_acc: 0.3784\n",
            "Epoch [14], train_loss: 1.4899, val_loss: 2.7519, val_acc: 0.3776\n",
            "Epoch [15], train_loss: 1.4115, val_loss: 2.8654, val_acc: 0.3742\n",
            "Epoch [16], train_loss: 1.3594, val_loss: 2.9538, val_acc: 0.3693\n",
            "Epoch [17], train_loss: 1.3087, val_loss: 3.0780, val_acc: 0.3560\n",
            "Epoch [18], train_loss: 1.2598, val_loss: 3.1963, val_acc: 0.3576\n",
            "Epoch [19], train_loss: 1.2028, val_loss: 3.2078, val_acc: 0.3620\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model2 = CifarCNN(\"GumbelSoftmax\")\n",
        "to_device(model2, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6VjJ0mNTXBTR",
        "outputId": "0aca56e6-29cd-43ce-c225-43eaebfd96af"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CifarCNN(\n",
              "  (network): Sequential(\n",
              "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): ReLU()\n",
              "    (2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (3): ReLU()\n",
              "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (6): ReLU()\n",
              "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (8): ReLU()\n",
              "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (11): ReLU()\n",
              "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (13): ReLU()\n",
              "    (14): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (15): Flatten(start_dim=1, end_dim=-1)\n",
              "    (16): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "    (17): ReLU()\n",
              "    (18): Linear(in_features=1024, out_features=512, bias=True)\n",
              "    (19): ReLU()\n",
              "    (20): Linear(in_features=512, out_features=100, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fit(model2, train_loader, val_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 328
        },
        "id": "umRLFjDDdBTK",
        "outputId": "abfe812b-1882-4fde-f355-f135be613174"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-1aea892898c5>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-7-e5f126f4906f>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(model, train_loader, val_loader, epochs, lr, opt_func)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mtrain_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m             \u001b[0mtrain_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-2fd1c795b1ae>\u001b[0m in \u001b[0;36mtrain_batch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mnorm_func\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"AdaptiveSoftmax\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0mnorm_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-ae019bed6766>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m       \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgumbel_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtau\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtemperature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat1 in method wrapper_CUDA_addmm)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model3 = CifarCNN(\"AdaptiveSoftmax\")\n",
        "to_device(model3, device)"
      ],
      "metadata": {
        "id": "o5UToUGPdDWn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c5f1470-7979-4d6d-87f2-af7b667eb9e8"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CifarCNN(\n",
              "  (network): Sequential(\n",
              "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): ReLU()\n",
              "    (2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (3): ReLU()\n",
              "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (6): ReLU()\n",
              "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (8): ReLU()\n",
              "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (11): ReLU()\n",
              "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (13): ReLU()\n",
              "    (14): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (15): Flatten(start_dim=1, end_dim=-1)\n",
              "    (16): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "    (17): ReLU()\n",
              "    (18): Linear(in_features=1024, out_features=512, bias=True)\n",
              "    (19): ReLU()\n",
              "    (20): Linear(in_features=512, out_features=100, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fit(model3, train_loader, test_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zWW0yy6tgUmj",
        "outputId": "77aa6e29-2e69-4a4c-ffeb-aafaa1f54e0f"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [0], train_loss: 5.8544, val_loss: 5.8468, val_acc: 0.0088\n",
            "Epoch [1], train_loss: 5.8543, val_loss: 5.8476, val_acc: 0.0088\n",
            "Epoch [2], train_loss: 5.8542, val_loss: 5.8485, val_acc: 0.0097\n",
            "Epoch [3], train_loss: 5.8538, val_loss: 5.8478, val_acc: 0.0088\n",
            "Epoch [4], train_loss: 5.8523, val_loss: 5.8471, val_acc: 0.0088\n",
            "Epoch [5], train_loss: 5.8548, val_loss: 5.8492, val_acc: 0.0088\n",
            "Epoch [6], train_loss: 5.8547, val_loss: 5.8501, val_acc: 0.0103\n",
            "Epoch [7], train_loss: 5.8558, val_loss: 5.8468, val_acc: 0.0103\n",
            "Epoch [8], train_loss: 5.8542, val_loss: 5.8499, val_acc: 0.0103\n",
            "Epoch [9], train_loss: 5.8522, val_loss: 5.8451, val_acc: 0.0106\n",
            "Epoch [10], train_loss: 5.8539, val_loss: 5.8468, val_acc: 0.0106\n",
            "Epoch [11], train_loss: 5.8539, val_loss: 5.8443, val_acc: 0.0103\n",
            "Epoch [12], train_loss: 5.8519, val_loss: 5.8494, val_acc: 0.0096\n",
            "Epoch [13], train_loss: 5.8551, val_loss: 5.8471, val_acc: 0.0096\n",
            "Epoch [14], train_loss: 5.8538, val_loss: 5.8480, val_acc: 0.0096\n",
            "Epoch [15], train_loss: 5.8550, val_loss: 5.8484, val_acc: 0.0106\n",
            "Epoch [16], train_loss: 5.8551, val_loss: 5.8498, val_acc: 0.0102\n",
            "Epoch [17], train_loss: 5.8542, val_loss: 5.8490, val_acc: 0.0102\n",
            "Epoch [18], train_loss: 5.8538, val_loss: 5.8480, val_acc: 0.0106\n",
            "Epoch [19], train_loss: 5.8539, val_loss: 5.8460, val_acc: 0.0106\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}