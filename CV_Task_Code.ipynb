{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "tut8M3YLn7Pc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4c66664-5c8c-433f-87e6-fb8353820ff4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting ftfy\n",
            "  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (2022.10.31)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.65.0)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.10/dist-packages (from ftfy) (0.2.6)\n",
            "Installing collected packages: ftfy\n",
            "Successfully installed ftfy-6.1.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/openai/CLIP.git\n",
            "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-tmextwzi\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-tmextwzi\n",
            "  Resolved https://github.com/openai/CLIP.git to commit a9b1bf5920416aaeaec965c25dd9e8f98c864f16\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (6.1.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (2022.10.31)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (4.65.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (2.0.1+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (0.15.2+cu118)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.10/dist-packages (from ftfy->clip==1.0) (0.2.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->clip==1.0) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->clip==1.0) (16.0.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (1.22.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (2.27.1)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (8.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->clip==1.0) (2.1.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip==1.0) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip==1.0) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip==1.0) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip==1.0) (3.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->clip==1.0) (1.3.0)\n",
            "Building wheels for collected packages: clip\n",
            "  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369370 sha256=2b966821243fc27691f6beb2a3c12a70882995ef611e994aff59adfb83732b56\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-yrimilhf/wheels/da/2b/4c/d6691fa9597aac8bb85d2ac13b112deb897d5b50f5ad9a37e4\n",
            "Successfully built clip\n",
            "Installing collected packages: clip\n",
            "Successfully installed clip-1.0\n"
          ]
        }
      ],
      "source": [
        "!pip install ftfy regex tqdm\n",
        "!pip install git+https://github.com/openai/CLIP.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "from os.path import basename, dirname, join, isfile\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from torch.nn.modules.activation import ReLU\n",
        "from torchvision import transforms\n",
        "import cv2\n",
        "import numpy as np\n",
        "import clip"
      ],
      "metadata": {
        "id": "esBHKcDF7k7D"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CLIPSegBaseEncoder(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "    # initialising clip as encoder\n",
        "    self.clip_model, self.preprocess = clip.load('ViT-B/32', device='cpu', jit=False)\n",
        "    self.model = self.clip_model.visual\n",
        "\n",
        "    # freeze the layers of clip\n",
        "    for p in self.clip_model.parameters():\n",
        "      p.requires_grad_(False)\n",
        "\n",
        "    # FiLM layers\n",
        "    self.film_mulitplier = nn.Linear(512, 128)\n",
        "    self.film_adder = nn.Linear(512, 128)\n",
        "\n",
        "    # linear layer for reducing dimensions\n",
        "    self.reduce = nn.Linear(768, 128)\n",
        "\n",
        "    # generating prompt variations\n",
        "    self.prompt_list = ['a photo of a {}.', 'a photograph of a {}.', 'an image of a {}.', '{}.', 'this picture is of a {}']\n",
        "\n",
        "  # forward pass to get activations\n",
        "  def visual_forward(self, x_input):\n",
        "    with torch.no_grad():\n",
        "      x = self.model.conv1(x_input) \n",
        "      x = x.reshape(x.shape[0], x.shape[1], -1)\n",
        "      x = x.permute(0, 2, 1)\n",
        "      x = torch.cat([self.model.class_embedding.to(x.dtype) + torch.zeros(x.shape[0], 1, x.shape[-1], dtype=x.dtype, device=x.device), x], dim=1)\n",
        "\n",
        "      # passing through first layer of visual encoder\n",
        "      x = self.model.ln_pre(x)\n",
        "\n",
        "      # a forward pass through the layers and obtaining activations\n",
        "      x = x.permute(1, 0, 2)\n",
        "      activations = []\n",
        "      for i, res_block in enumerate(self.model.transformer.resblocks):\n",
        "        x, aff = self.forward_mha(x, res_block)\n",
        "\n",
        "        # extracting the information\n",
        "        if (i == 2 or i == 5 or i == 8):\n",
        "          activations.append(x)\n",
        "\n",
        "      x = x.permute(1, 0, 2)\n",
        "      x = self.model.ln_post(x[:, 0, :])\n",
        "\n",
        "      # projection\n",
        "      x = x @ self.model.proj\n",
        "\n",
        "      return x, activations\n",
        "\n",
        "  # prompt generator\n",
        "  def sample_prompts(self, words):\n",
        "    prompt_indices = torch.multinomial(torch.ones(len(self.prompt_list)), len(words), replacement=True) # random sampling for prompt engineering        \n",
        "    prompts = [self.prompt_list[i] for i in prompt_indices]\n",
        "    return [p.format(w) for p, w in zip(prompts, words)]\n",
        "\n",
        "  # get encoded image and text prompt\n",
        "  def get_embeddings(self, image, text):\n",
        "    image_encoded = self.clip_mode.encode_image(image)\n",
        "    text_encoded = self.clip_model.encode_text(text)\n",
        "    return (image_encoded, text_encoded)\n",
        "\n",
        "  # multi head attention, modified to give attention outputs and weights\n",
        "  def forward_mha(self, x, block):\n",
        "    tmp_x = block.ln_1(x)\n",
        "    q, k, v = F.linear(tmp_x, block.attn.in_proj_weight, block.attn.in_proj_bias).chunk(3, dim=-1)\n",
        "    \n",
        "    target_length, batch_size, embed_dim = q.size()\n",
        "    heads = int(embed_dim / block.attn.num_heads)\n",
        "    scaling_factor = 1 / math.sqrt(embed_dim)\n",
        "\n",
        "    q = q.contiguous().view(target_length, batch_size * block.attn.num_heads, block.attn.head_dim).transpose(0, 1)\n",
        "    k = k.contiguous().view(-1, batch_size * block.attn.num_heads, block.attn.head_dim).transpose(0, 1)\n",
        "    v = v.contiguous().view(-1, batch_size * block.attn.num_heads, block.attn.head_dim).transpose(0, 1)\n",
        "\n",
        "    q = q * scaling_factor\n",
        "\n",
        "    attn_output_weights = torch.softmax(torch.bmm(q, k.transpose(1, 2)), dim=-1)\n",
        "    attn_output = torch.bmm(attn_output_weights, v).transpose(0, 1).contiguous().view(target_length, batch_size, embed_dim)\n",
        "    attn_output = block.attn.out_proj(attn_output)\n",
        "\n",
        "    x += block.mlp(block.ln_2(x + attn_output))\n",
        "\n",
        "    return x, attn_output_weights"
      ],
      "metadata": {
        "id": "qVcnyXis7lNG"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CLIPSeg(CLIPSegBaseEncoder):\n",
        "  \n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.add_activation1 = True\n",
        "    self.token_shape = (7, 7)\n",
        "    self.trans_conv_ks = (32, 32)\n",
        "\n",
        "    self.trans_conv = nn.ConvTranspose2d(128, 1, self.trans_conv_ks, stride=self.trans_conv_ks)\n",
        "\n",
        "    self.reduces = nn.ModuleList([nn.Linear(768, 128) for _ in range(3)])\n",
        "    self.blocks = nn.ModuleList([nn.TransformerEncoderLayer(d_model=128, nhead=4) for _ in range(3)])\n",
        "\n",
        "  def forward(self, input_image):\n",
        "    input_image = input_image.to(self.model.positional_embedding.device)\n",
        "    x_input = input_image\n",
        "    \n",
        "    batch_sz, device = input_image.shape[0], x_input.device\n",
        "\n",
        "    visual_query, activations = self.visual_forward(x_input)\n",
        "    \n",
        "    tmp = None\n",
        "    for i, (a, b, r) in enumerate(zip(activations[1:][::-1], self.blocks, self.reduces)):\n",
        "      if tmp is None : tmp = r(a)\n",
        "      else: tmp = r(a)+tmp\n",
        "      \n",
        "      # if i == 0:\n",
        "        # cond = self.reduce(cond)\n",
        "        # tmp = self.film_multiplier(cond) * tmp + self.film_adder(cond)\n",
        "      \n",
        "      tmp = b(tmp)\n",
        "\n",
        "    tmp = tmp[1:].permute(1, 2, 0)\n",
        "    size = int(math.sqrt(tmp.shape[2]))\n",
        "    tmp = tmp.view(batch_sz, tmp.shape[1], size, size)\n",
        "\n",
        "    tmp = self.trans_conv(tmp)\n",
        "\n",
        "    return tmp, visual_query, [activations[0]] + activations"
      ],
      "metadata": {
        "id": "fqc0HtWoEM9o"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = CLIPSeg()\n",
        "model.eval()\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "    transforms.Resize((352, 352)),\n",
        "])\n",
        "img_1 = transform(cv2.imread(\"/content/107900.jpg\"))\n",
        "img = img_1.unsqueeze(0)\n",
        "prompts = ['a picture of grass', 'an image of zebra', 'pic of hair', 'a photo of legs']\n",
        "\n",
        "preds = model(img)[0]\n",
        "preds"
      ],
      "metadata": {
        "id": "tS58DJiJ24SB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2dad7d5-cc2b-4874-c08e-0a82c22c8dd7"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[[ 0.1288,  0.3465,  0.0980,  ..., -0.2174,  0.4075, -0.1017],\n",
              "          [ 0.0583,  0.1025, -0.1517,  ...,  0.0401, -0.0200, -0.0403],\n",
              "          [-0.0386, -0.0158, -0.1168,  ...,  0.2481, -0.1347,  0.0204],\n",
              "          ...,\n",
              "          [-0.4754,  0.0299,  0.2080,  ..., -0.0223,  0.1423,  0.3054],\n",
              "          [-0.1150, -0.3350,  0.5264,  ...,  0.0036, -0.1412,  0.1400],\n",
              "          [ 0.1088,  0.2987, -0.2237,  ...,  0.0157, -0.0670,  0.2034]]]],\n",
              "       grad_fn=<ConvolutionBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# visualize prediction (fix shape of image)\n",
        "import matplotlib.pyplot as plt\n",
        "_, ax = plt.subplots(1, 5, figsize=(15, 4))\n",
        "[a.axis('off') for a in ax.flatten()]\n",
        "ax[0].imshow(img_1)\n",
        "[ax[i+1].imshow(torch.sigmoid(preds[i][0])) for i in range(4)];\n",
        "[ax[i+1].text(0, -15, prompts[i]) for i in range(4)];"
      ],
      "metadata": {
        "id": "dpO3ipGYjko9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 708
        },
        "outputId": "0eb6b8ec-3905-448a-fa93-f77eeeaf452b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-eee7eb9aeb95>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'off'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0max\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprompts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/__init__.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1440\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1442\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msanitize_sequence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1444\u001b[0m         \u001b[0mbound\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_sig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mimshow\u001b[0;34m(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, interpolation_stage, filternorm, filterrad, resample, url, **kwargs)\u001b[0m\n\u001b[1;32m   5663\u001b[0m                               **kwargs)\n\u001b[1;32m   5664\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5665\u001b[0;31m         \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5666\u001b[0m         \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_alpha\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5667\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_clip_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/image.py\u001b[0m in \u001b[0;36mset_data\u001b[0;34m(self, A)\u001b[0m\n\u001b[1;32m    708\u001b[0m         if not (self._A.ndim == 2\n\u001b[1;32m    709\u001b[0m                 or self._A.ndim == 3 and self._A.shape[-1] in [3, 4]):\n\u001b[0;32m--> 710\u001b[0;31m             raise TypeError(\"Invalid shape {} for image data\"\n\u001b[0m\u001b[1;32m    711\u001b[0m                             .format(self._A.shape))\n\u001b[1;32m    712\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Invalid shape (3, 352, 352) for image data"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1500x400 with 5 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "L6jct4uPsChP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}